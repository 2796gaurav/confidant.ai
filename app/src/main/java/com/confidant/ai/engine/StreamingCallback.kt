package com.confidant.ai.engine

/**
 * StreamingCallback - Interface for token-by-token streaming from native layer
 * 
 * Called by JNI as each token is generated by llama.cpp
 * Enables REAL streaming (not fake chunking)
 */
interface StreamingCallback {
    /**
     * Called for each generated token
     * @param token The text representation of the token
     */
    fun onToken(token: String)
    
    /**
     * Called when generation completes successfully
     */
    fun onComplete()
    
    /**
     * Called if an error occurs during generation
     * @param error Error message
     */
    fun onError(error: String)
}
